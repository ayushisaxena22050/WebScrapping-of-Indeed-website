{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=requests.get(\"https://www.indeed.co.in/jobs?q=data+scientist&l=Delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta http-equiv=\"content-type\" content=\"text/html\n"
     ]
    }
   ],
   "source": [
    "print(res.text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for extracting titles:\n",
    "def get_title(soup1):\n",
    "    title=[]\n",
    "    for div in soup1.findAll(name='div',attrs={'class':'row'}):\n",
    "        for a in div.find_all(name='a',attrs={'data-tn-element':'jobTitle'}):\n",
    "            title.append(a.get('title'))\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for extracting location:\n",
    "def get_location(soup1):\n",
    "    location=[]\n",
    "    for div in soup1.findAll(name='div',attrs={'class':'row'}):\n",
    "        for span in div.find_all(name='span',attrs={'class':'location'}):\n",
    "            location.append(span.get_text())\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for extracting summary:\n",
    "def get_summary(soup1):\n",
    "    summary=[]\n",
    "    for div in soup1.findAll(name='div',attrs={'class':'row'}):\n",
    "        for span in div.find_all(name='span',attrs={'class':'summary'}):\n",
    "            summary.append(span.get_text())\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for extracting company names\n",
    "def get_company(soup1):\n",
    "    company=[]\n",
    "    for div in soup1.findAll(name='div',attrs={'class':'row'}):\n",
    "        for span in div.find_all(name='span',attrs={'class':'company'}):\n",
    "            company.append(span.get_text())\n",
    "    return company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combing titles,location,summary,company together\n",
    "def combined(soup1):\n",
    "    title=get_title(soup1)\n",
    "    location=get_location(soup1)\n",
    "    summary=get_summary(soup1)\n",
    "    company=get_company(soup1)\n",
    "    arr=[title,location,summary,company]\n",
    "    info=pd.DataFrame(data=arr).transpose\n",
    "    info.columns=['Title','Location','Summary','Company']\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining scrapped data from different urls\n",
    "com=pd.DataFrame()\n",
    "urls=['https://www.indeed.co.in/jobs?q=data+scientist&l=Delhi','https://www.indeed.co.in/jobs?q=data+scientist&l=Delhi&start=10','https://www.indeed.co.in/jobs?q=data+scientist&l=Delhi&start=20','https://www.indeed.co.in/jobs?q=data+scientist&l=Delhi&start=30']\n",
    "for url in urls:\n",
    "    res=requests.get(url)\n",
    "    soup1=bs4.BeautifulSoup(res.text,'lxml')\n",
    "    a=combined(soup1)\n",
    "    b=pd.concat([com,a])\n",
    "    com = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorting scrapped data to a csv file\n",
    "b.to_csv('DataScientist_opening.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\181635\\appdata\\roaming\\python\\python37\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\181635\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from bs4) (4.8.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\181635\\appdata\\roaming\\python\\python37\\site-packages (from beautifulsoup4->bs4) (1.9.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\181635\\AppData\\Local\\Continuum\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
